#0
# -*- coding: utf-8 -*-
# Optuna íŒŒë¼ë¯¸í„° ì €ì¥/ë¡œë“œ ìœ ì§€, ì˜µíŠœë‚˜ 1íšŒ, fold ë°–ì—ì„œ 
# ì „ì²˜ë¦¬ ê°•í™” 6.5x BEST VERSION

import os
import json
import random
import warnings
import datetime
import numpy as np
import pandas as pd
import optuna

from sklearn.model_selection import KFold
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.linear_model import Ridge
from joblib import Parallel, delayed
from optuna.samplers import TPESampler
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, Conv1D, Flatten, Input, LSTM, Bidirectional
from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping
from sklearn.model_selection import RandomizedSearchCV
from tensorflow.keras.wrappers.scikit_learn import KerasRegressor
from sklearn.utils import class_weight
import tensorflow as tf
from xgboost import XGBRegressor
from tensorflow.keras.optimizers import Adam

warnings.filterwarnings("ignore")
# ==============================
# 0) ì‹œë“œ / ê²½ë¡œ
# ==============================
seed = 6054
random.seed(seed)
np.random.seed(seed)
tf.random.set_seed(seed)

path = 'C:/Study25/competition_ì „ë ¥/'
# ë°ì´í„° ë¡œë“œ
buildinginfo = pd.read_csv(path + 'building_info.csv')
train = pd.read_csv(path + 'train.csv')
test = pd.read_csv(path + 'test.csv')
samplesub = pd.read_csv(path + 'sample_submission.csv')

# === 0) ì˜µì…˜: building_info ë³‘í•© (ìˆìœ¼ë©´ ë³‘í•©, ì—†ìœ¼ë©´ ë„˜ì–´ê°)
have_bi = 'buildinginfo' in globals() or 'building_info' in globals()
if 'buildinginfo' in globals():
    bi = buildinginfo.copy()
else:
    bi = None

if bi is not None:
    # ì„¤ë¹„ ìš©ëŸ‰ì€ '-' â†’ 0, ìˆ«ìë¡œ ìºìŠ¤íŒ…
    for col in ['íƒœì–‘ê´‘ìš©ëŸ‰(kW)', 'ESSì €ì¥ìš©ëŸ‰(kWh)', 'PCSìš©ëŸ‰(kW)', 'ì—°ë©´ì (m2)', 'ëƒ‰ë°©ë©´ì (m2)']:
        if col in bi.columns:
            bi[col] = pd.to_numeric(bi[col].replace('-', np.nan), errors='coerce').fillna(0.0)
    # ì„¤ë¹„ ìœ ë¬´ í”Œë˜ê·¸
    bi['íƒœì–‘ê´‘_ìœ ë¬´'] = ((bi.get('íƒœì–‘ê´‘ìš©ëŸ‰(kW)', 0.0).astype(float) > 0).astype(int)) if 'íƒœì–‘ê´‘ìš©ëŸ‰(kW)' in bi.columns else 0
    bi['ESS_ìœ ë¬´']  = ((bi.get('ESSì €ì¥ìš©ëŸ‰(kWh)', 0.0).astype(float) > 0).astype(int)) if 'ESSì €ì¥ìš©ëŸ‰(kWh)' in bi.columns else 0

    # í•„ìš”í•œ ì»¬ëŸ¼ë§Œ ì¶”ë ¤ ë³‘í•© (ì—†ìœ¼ë©´ ìŠ¤í‚µ)
    keep_cols = ['ê±´ë¬¼ë²ˆí˜¸']
    for c in ['ê±´ë¬¼ìœ í˜•','ì—°ë©´ì (m2)','ëƒ‰ë°©ë©´ì (m2)','íƒœì–‘ê´‘ìš©ëŸ‰(kW)','ESSì €ì¥ìš©ëŸ‰(kWh)','PCSìš©ëŸ‰(kW)','íƒœì–‘ê´‘_ìœ ë¬´','ESS_ìœ ë¬´']:
        if c in bi.columns: keep_cols.append(c)
    bi = bi[keep_cols].drop_duplicates('ê±´ë¬¼ë²ˆí˜¸')

    train = train.merge(bi, on='ê±´ë¬¼ë²ˆí˜¸', how='left')
    test  = test.merge(bi, on='ê±´ë¬¼ë²ˆí˜¸',  how='left')

# === 1) ê³µí†µ ì‹œê°„ íŒŒìƒ
def add_time_features_kor(df: pd.DataFrame) -> pd.DataFrame:
    df = df.copy()
    df['ì¼ì‹œ'] = pd.to_datetime(df['ì¼ì‹œ'], format='%Y%m%d %H')
    df['hour']      = df['ì¼ì‹œ'].dt.hour
    df['day']       = df['ì¼ì‹œ'].dt.day
    df['month']     = df['ì¼ì‹œ'].dt.month
    df['dayofweek'] = df['ì¼ì‹œ'].dt.dayofweek
    df['is_weekend']      = (df['dayofweek'] >= 5).astype(int)
    df['is_working_hours'] = ((df['hour'] >= 9) & (df['hour'] <= 18)).astype(int)
    df['sin_hour']  = np.sin(2*np.pi*df['hour']/24)
    df['cos_hour']  = np.cos(2*np.pi*df['hour']/24)
    df['sin_month'] = np.sin(2*np.pi*df['month']/12)
    df['cos_month'] = np.cos(2*np.pi*df['month']/12)
    df['sin_dow']   = np.sin(2*np.pi*(df['dayofweek']+1)/7.0)
    df['cos_dow']   = np.cos(2*np.pi*(df['dayofweek']+1)/7.0)
    if {'ê¸°ì˜¨(Â°C)','ìŠµë„(%)'}.issubset(df.columns):
        t = df['ê¸°ì˜¨(Â°C)']; h = df['ìŠµë„(%)']
        df['DI'] = 9/5*t - 0.55*(1 - h/100.0)*(9/5*t - 26) + 32
    else:
        df['DI'] = 0.0
    return df

train = add_time_features_kor(train)
test  = add_time_features_kor(test)

# === 2) expected_solar (train ê¸°ì¤€ â†’ ë‘˜ ë‹¤ì— ë¨¸ì§€)
if 'ì¼ì‚¬(MJ/m2)' in train.columns:
    solar_proxy = (
        train.groupby(['month','hour'])['ì¼ì‚¬(MJ/m2)']
             .mean().reset_index()
             .rename(columns={'ì¼ì‚¬(MJ/m2)':'expected_solar'})
    )
    train = train.merge(solar_proxy, on=['month','hour'], how='left')
    test  = test.merge(solar_proxy,  on=['month','hour'], how='left')
else:
    train['expected_solar'] = 0.0
    test['expected_solar']  = 0.0

train['expected_solar'] = train['expected_solar'].fillna(0)
test['expected_solar']  = test['expected_solar'].fillna(0)

# === 3) ì¼ë³„ ì˜¨ë„ í†µê³„ (train/test ë™ì¼ ë¡œì§)
def add_daily_temp_stats_kor(df: pd.DataFrame) -> pd.DataFrame:
    df = df.copy()
    if 'ê¸°ì˜¨(Â°C)' not in df.columns:
        for c in ['day_max_temperature','day_mean_temperature','day_min_temperature','day_temperature_range']:
            df[c] = 0.0
        return df
    grp = df.groupby(['ê±´ë¬¼ë²ˆí˜¸','month','day'])['ê¸°ì˜¨(Â°C)']
    stats = grp.agg(day_max_temperature='max',
                      day_mean_temperature='mean',
                      day_min_temperature='min').reset_index()
    df = df.merge(stats, on=['ê±´ë¬¼ë²ˆí˜¸','month','day'], how='left')
    df['day_temperature_range'] = df['day_max_temperature'] - df['day_min_temperature']
    return df

train = add_daily_temp_stats_kor(train)
test  = add_daily_temp_stats_kor(test)

# === 4) CDH / THI / WCT (train/test ë™ì¼)
def add_CDH_kor(df: pd.DataFrame) -> pd.DataFrame:
    df = df.copy()
    if 'ê¸°ì˜¨(Â°C)' not in df.columns:
        df['CDH'] = 0.0
        return df
    def _cdh_1d(x):
        cs = np.cumsum(x - 26)
        return np.concatenate((cs[:11], cs[11:] - cs[:-11])) if len(x) >= 12 else np.zeros_like(x, dtype=float)
    parts = []
    for bno, g in df.sort_values('ì¼ì‹œ').groupby('ê±´ë¬¼ë²ˆí˜¸'):
        arr = g['ê¸°ì˜¨(Â°C)'].to_numpy()
        cdh = _cdh_1d(arr)
        parts.append(pd.Series(cdh, index=g.index))
    df['CDH'] = pd.concat(parts).sort_index()
    return df

def add_THI_WCT_kor(df: pd.DataFrame) -> pd.DataFrame:
    df = df.copy()
    if {'ê¸°ì˜¨(Â°C)','ìŠµë„(%)'}.issubset(df.columns):
        t = df['ê¸°ì˜¨(Â°C)']; h = df['ìŠµë„(%)']
        df['THI'] = 9/5*t - 0.55*(1 - h/100.0)*(9/5*t - 26) + 32
    else:
        df['THI'] = 0.0
    if {'ê¸°ì˜¨(Â°C)','í’ì†(m/s)'}.issubset(df.columns):
        t = df['ê¸°ì˜¨(Â°C)']; w = df['í’ì†(m/s)'].clip(lower=0)
        df['WCT'] = 13.12 + 0.6125*t - 11.37*(w**0.16) + 0.3965*(w**0.16)*t
    else:
        df['WCT'] = 0.0
    return df

train = add_CDH_kor(train)
test  = add_CDH_kor(test)
train = add_THI_WCT_kor(train)
test  = add_THI_WCT_kor(test)

# === 5) ì‹œê°„ëŒ€ ì „ë ¥ í†µê³„ (trainìœ¼ë¡œ ê³„ì‚° â†’ ë‘˜ ë‹¤ merge)
if 'ì „ë ¥ì†Œë¹„ëŸ‰(kWh)' in train.columns:
    pm = (train
          .groupby(['ê±´ë¬¼ë²ˆí˜¸','hour','dayofweek'])['ì „ë ¥ì†Œë¹„ëŸ‰(kWh)']
          .agg(['mean','std'])
          .reset_index()
          .rename(columns={'mean':'day_hour_mean','std':'day_hour_std'}))
    train = train.merge(pm, on=['ê±´ë¬¼ë²ˆí˜¸','hour','dayofweek'], how='left')
    test  = test.merge(pm,  on=['ê±´ë¬¼ë²ˆí˜¸','hour','dayofweek'],  how='left')
else:
    train['day_hour_mean'] = 0.0; train['day_hour_std'] = 0.0
    test['day_hour_mean']  = 0.0; test['day_hour_std']  = 0.0

# === 6) (ì„ íƒ) ì´ìƒì¹˜ ì œê±°: 0 kWh ì œê±°
if 'ì „ë ¥ì†Œë¹„ëŸ‰(kWh)' in train.columns:
    train = train.loc[train['ì „ë ¥ì†Œë¹„ëŸ‰(kWh)'] > 0].reset_index(drop=True)

# === 7) ë²”ì£¼í˜• ê±´ë¬¼ìœ í˜• ì¸ì½”ë”© (ìˆì„ ë•Œë§Œ)
if 'ê±´ë¬¼ìœ í˜•' in train.columns and 'ê±´ë¬¼ìœ í˜•' in test.columns:
    both = pd.concat([train['ê±´ë¬¼ìœ í˜•'], test['ê±´ë¬¼ìœ í˜•']], axis=0).astype('category')
    cat_map = {cat: i for i, cat in enumerate(both.cat.categories)}
    train['ê±´ë¬¼ìœ í˜•'] = train['ê±´ë¬¼ìœ í˜•'].map(cat_map).fillna(-1).astype(int)
    test['ê±´ë¬¼ìœ í˜•']  = test['ê±´ë¬¼ìœ í˜•'].map(cat_map).fillna(-1).astype(int)
    
# 1) ê³µí†µ feature (train/test ë‘˜ ë‹¤ ìˆëŠ” ì»¬ëŸ¼ë§Œ ì„ íƒ)
feature_candidates = [
    'ê±´ë¬¼ìœ í˜•','ì—°ë©´ì (m2)','ëƒ‰ë°©ë©´ì (m2)','íƒœì–‘ê´‘ìš©ëŸ‰(kW)','ESSì €ì¥ìš©ëŸ‰(kWh)','PCSìš©ëŸ‰(kW)','íƒœì–‘ê´‘_ìœ ë¬´','ESS_ìœ ë¬´',
    'ê¸°ì˜¨(Â°C)','ê°•ìˆ˜ëŸ‰(mm)','í’ì†(m/s)','ìŠµë„(%)','ì¼ì¡°(hr)','ì¼ì‚¬(MJ/m2)',
    'hour','day','month','dayofweek','is_weekend','is_working_hours',
    'sin_hour','cos_hour','sin_month','cos_month','sin_dow','cos_dow',
    'DI','expected_solar',
    'day_max_temperature','day_mean_temperature','day_min_temperature','day_temperature_range',
    'CDH','THI','WCT',
    'day_hour_mean','day_hour_std'
]
features = [c for c in feature_candidates if c in train.columns and c in test.columns]

# 2) target ëª…ì‹œ
target = 'ì „ë ¥ì†Œë¹„ëŸ‰(kWh)'
if target not in train.columns:
    raise ValueError(f"train ë°ì´í„°ì— target ì»¬ëŸ¼({target})ì´ ì—†ìŠµë‹ˆë‹¤!")

# 3) ìµœì¢… ì…ë ¥/íƒ€ê¹ƒ ë°ì´í„°
X = train[features].values
y = np.log1p(train[target].values.astype(float))
X_test_raw = test[features].values
ts = train['ì¼ì‹œ']  # ë‚´ë¶€ CVì—ì„œ ì •ë ¬/ì°¸ì¡°ìš© ê°€ëŠ¥

print(f"[í™•ì¸] ì‚¬ìš© features ê°œìˆ˜: {len(features)}")
print(f"[í™•ì¸] target: {target}")
print(f"[í™•ì¸] X shape: {X.shape}, X_test shape: {X_test_raw.shape}, y shape: {y.shape}")

# ------------------------------
# SMAPE
# ------------------------------
def smape_exp(y_true_log, y_pred_log):
    y_true = np.expm1(y_true_log)
    y_pred = np.expm1(y_pred_log)
    return np.mean(200 * np.abs(y_pred - y_true) / (np.abs(y_pred) + np.abs(y_true) + 1e-6))

# ------------------------------
# [ë³€ê²½] Optuna: ë‚´ë¶€ CV(KFold 3, shuffle=True)ë¡œ "ê±´ë¬¼ë‹¹ 1íšŒ" íŠœë‹
# ------------------------------
def create_hyperparameter():
    batchs =[16,12,8,6]
    optimizers = ['adam','rmsprop','adadelta']
    dropouts = [0.2, 0.3, 0.4, 0.5]
    activations = ['relu','selu',]
    node1 = [128,64,32,16]
    node2 = [128,64,32,16]
    node3 = [128,64,32,16]
    node4 = [128,64,32,16]
    node5 = [128,64,32,16, 8]
    return{
        'batch_size' : batchs,
        'optimizer' : optimizers,
        'drop' : dropouts,
        'activation': activations,
        'node1':node1,
        'node2':node2,
        'node3':node3,
        'node4':node4,
        'node5':node5,
        
    }

es = EarlyStopping(
    monitor='val_loss', # í‰ê°€ ì§€í‘œë¡œ í™•ì¸í•˜ê² ë‹¤
    mode= 'min', # ìµœëŒ€ê°’ max, ì•Œì•„ì„œ ì°¾ì•„ì¤˜:auto
    patience=50, # 10ë²ˆê¹Œì§€ ì´ˆê³¼í•´ë„ ë„˜ì–´ê°€ê² ë‹¤
    restore_best_weights= True # val_lossê°’ì´ ê°€ì¥ ë‚®ì€ ê°’ìœ¼ë¡œ ì €ì¥ í•´ë†“ê² ë‹¤(Falseì‹œ => ìµœì†Œê°’ ì´í›„ 10ë²ˆì§¸ ê°’ìœ¼ë¡œ ê·¸ëƒ¥ ì¡ëŠ”ë‹¤.)
)
rlr = ReduceLROnPlateau( # ëª¨ë¸ì´ í›ˆë ¨í•˜ë©´ì„œ Learning Rateë¥¼ ì¡°ì ˆ í• ë•Œ ì“°ì„.
monitor='val_loss',
mode='auto',
patience= 30,
verbose=1,
factor=0.5, # Learning Rate *0.5 ì”© ë³€í™”ë¥¼ ì¤Œ(ì—¬ê¸°ì„œëŠ” ë‚®ì•„ì§)
)

def build_model(drop=0.5, optimizer='adam', activation='relu',
                node1=128, node2=64,node3=32, node4=16, node5=8):#lr=0.001):
      #2. ëª¨ë¸êµ¬ì„±
    inputs = Input(shape=(24,135),name='inputs')
    x = Conv1D(node1, kernel_size=3,activation=activation, name='conv1D')(inputs)
    x = Conv1D(node2, kernel_size=3,activation=activation, name='conv1D_1')(x)
    x = BatchNormalization()(x)
    x = Dropout(drop)(x)
    x = Bidirectional(LSTM(32))(x)
    # x = Flatten()(x)
    x = Dense(node3, activation=activation)(x)
    x= BatchNormalization()(x)
    x = Dropout(drop)(x)
    x = Dense(node4, activation=activation)(x)
    x = BatchNormalization()(x)
    x = Dropout(drop)(x)
    x = Dense(node5, activation=activation)(x)
    x = BatchNormalization()(x)
    outputs = Dense(1,activation='linear',name='outputs')(x)
    model = Model(inputs=inputs, outputs=outputs)
      # ì˜µí‹°ë§ˆì´ì € ì„¤ì •
    if optimizer == 'adam':
        model.compile(optimizer=Adam(learning_rate=0.01), metrics=['mae'], loss='mse')
    else:
        model.compile(optimizer=optimizer, metrics=['mae'], loss='mse')

    return model
# ê¸°ë³¸ ì„¸íŒ…
timesteps = 24
target_horizon = 0
stride = 1
# ìŠ¬ë¼ì´ë”© ìœˆë„ìš° í•¨ìˆ˜
def split_xy_stride(X, y=None, window_size=24, stride=1):
    X_seqs = []
    y_seqs = []
    
    for i in range(0, len(X) - window_size + 1, stride):
        X_seq = X[i:i + window_size]
        
        if y is not None:  # yê°€ Noneì´ ì•„ë‹ ë•Œë§Œ yë¥¼ ì²˜ë¦¬
            y_seq = y[i + window_size + target_horizon - 1]
            y_seqs.append(y_seq)
        
        X_seqs.append(X_seq)
    
    if y is not None:
        return np.array(X_seqs), np.array(y_seqs)
    else:
        return np.array(X_seqs)  # X_testì¼ ê²½ìš° yë¥¼ ë°˜í™˜í•˜ì§€ ì•ŠìŒ

def process_building_kfold():

    # 1. ì›í•« ì¸ì½”ë”©
    # sparse=Falseë¡œ ë³€ê²½
    encoder = OneHotEncoder(sparse=False)
    building_encoded = encoder.fit_transform(train[["ê±´ë¬¼ë²ˆí˜¸"]])
    building_encoded_test = encoder.transform(test[["ê±´ë¬¼ë²ˆí˜¸"]])
    
    # 2. ì›í•« ì¸ì½”ë”©ëœ ê±´ë¬¼ë²ˆí˜¸ë¥¼ ê¸°ì¡´ featuresì— ì¶”ê°€
    X_full = np.concatenate([train[features].values, building_encoded], axis=1)
    y_full = np.log1p(train[target].values.astype(float))
    X_test = np.concatenate([test[features].values, building_encoded_test], axis=1)

    # ë©”ëª¨ë¦¬ ë¬¸ì œ í•´ê²°ì„ ìœ„í•´ float32ë¡œ ë°ì´í„° íƒ€ì… ë³€ê²½
    X_full = X_full.astype(np.float32)
    y_full = y_full.astype(np.float32)
    X_test = X_test.astype(np.float32)
    
    sc = StandardScaler()
    X_full = sc.fit_transform(X_full) 
    X_test = sc.transform(X_test)
    
    X_full, y_full = split_xy_stride(X_full, y_full, window_size=timesteps, stride=stride)
    X_test = split_xy_stride(X_test, y=None, window_size=timesteps, stride=stride)

    kf = KFold(n_splits=5, shuffle=True, random_state=seed)  # ì™¸ë¶€ KFoldëŠ” ê·¸ëŒ€ë¡œ ìœ ì§€
    test_preds, val_smapes = [], []

    for fold, (tr_idx, va_idx) in enumerate(kf.split(X_full), 1):
        print(f" - fold {fold}")
        X_tr, X_va = X_full[tr_idx], X_full[va_idx]
        y_tr, y_va = y_full[tr_idx], y_full[va_idx]

        X_tr_s = X_tr
        X_va_s = X_va
        X_te_s = X_test
        
        hyperparameters = create_hyperparameter()
        
        #3. ì»´íŒŒì¼, í›ˆë ¨
        keras_model = KerasRegressor(build_fn=build_model, verbose=1)

        model = RandomizedSearchCV(keras_model, hyperparameters, cv=3, #kerasë¥¼ sklearnì— ë©í•‘í•œê±¸ë¡œ ë‹¬ë¼
                                 n_iter=50,
                                 verbose=1,
                                 refit=True
                                 )
        
        hist = model.fit(X_tr_s, y_tr,epochs= 40, batch_size= 256,verbose=1,validation_split=0.1,callbacks=[es,rlr])
        best_params = model.best_params_
        # 1. ìµœì ì˜ íŒŒë¼ë¯¸í„°ë¥¼ ë¡œê·¸ë¡œ ê¸°ë¡
        print(f"Best Parameters for fold {fold}: {best_params}")
        
        # 2. ìµœì ì˜ íŒŒë¼ë¯¸í„°ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥ (ì„ íƒ ì‚¬í•­)
        with open(f"best_params_fold_{fold}.json", "w") as f:
            json.dump(best_params, f, indent=4)
        
          # ìŠ¤íƒœí‚¹ì„ ìœ„í•œ oof
        oof_tr = hist.predict(X_tr_s)
        oof_va =  hist.predict(X_va_s)
        oof_te = hist.predict(X_te_s)  
        
        # Conv1D ì˜ˆì¸¡ê°’ (None, 24, 135)ì„ (None, 24*135)ë¡œ ë³€í™˜
        # (N, T, F) -> (N, T*F) í˜•íƒœë¡œ ë³€í™˜
        oof_tr_flat = oof_tr.reshape(oof_tr.shape[0], -1) 
        oof_va_flat = oof_va.reshape(oof_va.shape[0], -1)  
        oof_te_flat = oof_te.reshape(oof_te.shape[0], -1)  
        
        # 3. XGBRegressorì— Conv1D ì˜ˆì¸¡ê°’ì„ í”¼ì²˜ë¡œ ì‚¬ìš©í•˜ì—¬ í›ˆë ¨
        xgb_model = XGBRegressor(n_estimators=200, learning_rate=0.005, max_depth=3)
        # Conv1D ì˜ˆì¸¡ê°’ì„ XGBì˜ ìƒˆë¡œìš´ í”¼ì²˜ë¡œ ì‚¬ìš©í•˜ì—¬ í›ˆë ¨
        xgb_model.fit(oof_tr_flat, y_tr)
        va_pred = xgb_model.predict(oof_va_flat)  # ì˜ˆì¸¡
        te_pred = xgb_model.predict(oof_te_flat)  # ì˜ˆì¸¡

        fold_smape = smape_exp(y_va, va_pred)
        val_smapes.append(fold_smape)
        test_preds.append(np.expm1(te_pred))  # ì—­ë¡œê·¸

    avg_test_pred = np.mean(test_preds, axis=0)
    avg_smape = float(np.mean(val_smapes)) if len(val_smapes) else np.nan
    return avg_test_pred.tolist(), avg_smape

# ==============================
# 12) ë³‘ë ¬ ì‹¤í–‰
# ==============================
# ëª¨ë¸ í›ˆë ¨ ë° í‰ê°€
final_preds, val_smapes = process_building_kfold()
# ê²°ê³¼ë¥¼ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸

samplesub["answer"] = final_preds
today = datetime.datetime.now().strftime("%Y%m%d")
avg_smape = float(np.mean(val_smapes))
filename = f"submission_stack_optuna_stdcols_{today}_SMAPE_{avg_smape:.4f}_{seed}.csv"
samplesub.to_csv(os.path.join(path, filename), index=False)

print(f"\nâœ… í‰ê·  SMAPE (ì „ì²´ ê±´ë¬¼): {avg_smape:.4f}")
print(f"ğŸ“ ì €ì¥ ì™„ë£Œ â†’ {filename}")